{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate git+https://github.com/TransformerLensOrg/TransformerLens git+https://github.com/neelnanda-io/neel-plotly.git  kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer, ActivationCache, patching, HookedTransformerConfig\n",
    "import transformer_lens\n",
    "from transformer_lens.patching import (\n",
    "    generic_activation_patch,\n",
    "    get_act_patch_resid_pre,\n",
    "    get_act_patch_attn_out,\n",
    "    get_act_patch_mlp_out,\n",
    ")\n",
    "from transformer_lens.ActivationCache import ActivationCache\n",
    "from transformer_lens.utils import *\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "from accelerate import init_empty_weights\n",
    "from typing import List, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from neel_plotly import line, imshow, scatter\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24037/3054465708.py:17: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_24037/3054465708.py:18: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
    "DEBUG_MODE = False\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "    %pip install git+https://github.com/TransformerLensOrg/TransformerLens.git\n",
    "    # Install my janky personal plotting utils\n",
    "    %pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "\n",
    "if IN_COLAB or not DEBUG_MODE:\n",
    "    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"colab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "print(accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_name = 'gpt2-small'\n",
    "# model_name = \"google/gemma-2-2b-it\"\n",
    "# mdoel_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f9e0c144cb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [04:19<00:00, 129.50s/it]\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-3B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(model_name \n",
    "                                        #   move_to_device=True, \n",
    "                                        #   n_devices=torch.cuda.device_count()\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cfg.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"What is 1234567 + 7654321?\", \"What is 8326542 + 1673345?\", \"What is 3785816 + 6214087?\"]\n",
    "answers = [(\"8888888\", \"9999999\"), (\"9999887\", \"1235789\"), (\"9999903\", \"2170830\")]\n",
    "prompts_c = [\"What is 7654321 + 1234567?\", \"What is 1673345 + 8326542?\", \"What is 6214087 + 3785816?\"]\n",
    "PAD_TOKEN = model.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenize Prompts and Answers\n",
    "clean_tokens = model.to_tokens(prompts)\n",
    "\n",
    "# Determine the device of the model's tokens\n",
    "device = clean_tokens.device\n",
    "\n",
    "# Tokenize correct and incorrect answers, and ensure token count consistency\n",
    "answer_token_indices = []\n",
    "for correct_answer, incorrect_answer in answers:\n",
    "    # Tokenize both correct and incorrect answers\n",
    "    correct_tokens = model.to_tokens(correct_answer)[0].to(device)\n",
    "    incorrect_tokens = model.to_tokens(incorrect_answer)[0].to(device)\n",
    "    \n",
    "    # Ensure they have the same number of tokens\n",
    "    if len(correct_tokens) < len(incorrect_tokens):\n",
    "        # Pad the correct tokens\n",
    "        correct_tokens = torch.cat(\n",
    "            [correct_tokens, torch.full((len(incorrect_tokens) - len(correct_tokens),), PAD_TOKEN, dtype=torch.long, device=device)]\n",
    "        )\n",
    "    elif len(correct_tokens) > len(incorrect_tokens):\n",
    "        # Pad the incorrect tokens\n",
    "        incorrect_tokens = torch.cat(\n",
    "            [incorrect_tokens, torch.full((len(correct_tokens) - len(incorrect_tokens),), PAD_TOKEN, dtype=torch.long, device=device)]\n",
    "        )\n",
    "\n",
    "    # Append tokenized answers to the list\n",
    "    answer_token_indices.append((correct_tokens, incorrect_tokens))\n",
    "\n",
    "# Step 3: Create Corrupted Tokens\n",
    "# Replace the correct answer tokens with the incorrect answer tokens\n",
    "corrupted_tokens = model.to_tokens(prompts_o).to(device)\n",
    "\n",
    "for i, (correct_tokens, incorrect_tokens) in enumerate(answer_token_indices):\n",
    "    # Locate where the model outputs the answer in the sequence\n",
    "    answer_start_idx = -len(correct_tokens)  # Assuming the answer is at the end of the sequence\n",
    "    answer_end_idx = corrupted_tokens.shape[1]\n",
    "\n",
    "    # Replace the correct answer tokens with the incorrect answer tokens\n",
    "    corrupted_tokens[i, answer_start_idx:answer_end_idx] = incorrect_tokens\n",
    "\n",
    "# Step 4: Run the Model\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arithmetic_metric(logits, answer_token_indices, corrupted_baseline, clean_baseline):\n",
    "    \"\"\"\n",
    "    Compute the arithmetic metric to evaluate the model's performance on arithmetic tasks.\n",
    "\n",
    "    Args:\n",
    "        logits: The model's output logits (shape: [batch_size, seq_len, vocab_size]).\n",
    "        answer_token_indices: A list of tuples [(correct_tokens, incorrect_tokens)] where:\n",
    "                              - correct_tokens: Token IDs for the correct answer (tensor of shape [seq_len_correct]).\n",
    "                              - incorrect_tokens: Token IDs for the incorrect answer (tensor of shape [seq_len_incorrect]).\n",
    "        corrupted_baseline: Metric value for the corrupted baseline (0).\n",
    "        clean_baseline: Metric value for the clean baseline (1).\n",
    "\n",
    "    Returns:\n",
    "        Metric value as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    batch_size = logits.shape[0]\n",
    "    total_logit_diffs = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Extract logits for the final positions (where the answers are expected)\n",
    "        correct_tokens = answer_token_indices[i][0]\n",
    "        incorrect_tokens = answer_token_indices[i][1]\n",
    "\n",
    "        # Get the logits for the correct and incorrect answers\n",
    "        correct_logits = logits[i, -len(correct_tokens):, :].gather(1, correct_tokens.unsqueeze(1)).sum()\n",
    "        incorrect_logits = logits[i, -len(incorrect_tokens):, :].gather(1, incorrect_tokens.unsqueeze(1)).sum()\n",
    "\n",
    "        # Calculate the logit difference for this example\n",
    "        logit_diff = correct_logits - incorrect_logits\n",
    "        total_logit_diffs.append(logit_diff)\n",
    "\n",
    "    # Average the logit differences across the batch\n",
    "    avg_logit_diff = torch.stack(total_logit_diffs).mean()\n",
    "\n",
    "    # Normalize the metric between corrupted and clean baselines\n",
    "    return (avg_logit_diff - corrupted_baseline) / (clean_baseline - corrupted_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the wrapper function for arithmetic_metric\n",
    "def arithmetic_metric_wrapper(logits):\n",
    "    return arithmetic_metric(\n",
    "        logits,\n",
    "        answer_token_indices=answer_token_indices,\n",
    "        corrupted_baseline=CORRUPTED_BASELINE,\n",
    "        clean_baseline=CLEAN_BASELINE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted Baseline: -5.9844\n",
      "Clean Baseline: -1.6410\n"
     ]
    }
   ],
   "source": [
    "# Compute baselines\n",
    "CORRUPTED_BASELINE = arithmetic_metric(corrupted_logits, answer_token_indices, 0, 1)\n",
    "CLEAN_BASELINE = arithmetic_metric(clean_logits, answer_token_indices, 0, 1)\n",
    "\n",
    "print(f\"Corrupted Baseline: {CORRUPTED_BASELINE:.4f}\")\n",
    "print(f\"Clean Baseline: {CLEAN_BASELINE:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364/364 [00:51<00:00,  7.04it/s]\n",
      "100%|██████████| 364/364 [00:51<00:00,  7.03it/s]\n",
      "100%|██████████| 364/364 [00:51<00:00,  7.05it/s]\n"
     ]
    }
   ],
   "source": [
    "every_block_result = patching.get_act_patch_block_every(\n",
    "    model, corrupted_tokens, clean_cache, arithmetic_metric_wrapper\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imshow(every_block_result, \n",
    "        facet_col=0, \n",
    "        facet_labels=[\"Residual Stream\", \"Attn Output\", \"MLP Output\"], \n",
    "        title=\"Activation Patching Per Block\", \n",
    "        xaxis=\"Position\",\n",
    "        yaxis=\"Layer\", \n",
    "        zmax=1, \n",
    "        zmin=-1, \n",
    "        x=[f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
    "        return_fig=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.write_image('./fig_o1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 84/672 [00:13<01:30,  6.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [01:43<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "attn_head_out_all_pos_act_patch_results = patching.get_act_patch_attn_head_out_all_pos(model, corrupted_tokens, clean_cache, arithmetic_metric_wrapper)\n",
    "img2 = imshow(attn_head_out_all_pos_act_patch_results, \n",
    "       yaxis=\"Layer\", \n",
    "       xaxis=\"Head\", \n",
    "       title=\"attn_head_out Activation Patching (All Pos)\",\n",
    "       return_fig=True\n",
    "    )\n",
    "img2.write_image('./fig_o2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 186/8736 [00:28<21:49,  6.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8736/8736 [22:31<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "ALL_HEAD_LABELS = [f\"L{i}H{j}\" for i in range(model.cfg.n_layers) for j in range(model.cfg.n_heads)]\n",
    "\n",
    "attn_head_out_act_patch_results = patching.get_act_patch_attn_head_out_by_pos(model, corrupted_tokens, clean_cache, arithmetic_metric_wrapper)\n",
    "attn_head_out_act_patch_results = einops.rearrange(attn_head_out_act_patch_results, \"layer pos head -> (layer head) pos\")\n",
    "img3 = imshow(attn_head_out_act_patch_results, \n",
    "        yaxis=\"Head Label\", \n",
    "        xaxis=\"Pos\", \n",
    "        x=[f\"{tok} {i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
    "        y=ALL_HEAD_LABELS,\n",
    "        title=\"attn_head_out Activation Patching By Pos\", \n",
    "        return_fig=True\n",
    "    )\n",
    "img3.write_image('./fig_o3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
